\documentclass{beamer}

\title{Modelos Generativos Profundos}
\subtitle{Clase 6: Implementación de un modelo GPT}
\author{Fernando Fêtis Riquelme}
\institute{
    Facultad de Ciencias Físicas y Matemáticas\\
    Universidad de Chile
}
\date{Otoño, 2025}
\titlegraphic{\hfill\includegraphics[height=1.2cm]{fcfm}}

\usetheme{metropolis}
\setbeamercovered{transparent}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}{Clase de hoy}
    \tableofcontents
\end{frame}

\section{Recuerdo clase anterior}

\begin{frame}{Recuerdo clase anterior}
    \begin{itemize}
        \item<1> Tokenización.
        \item<2> Creación de un dataset.
        \item<3> Entrenamiento.
        \item<4> Generación.
    \end{itemize}
\end{frame}

\section{Modelo GPT}

\begin{frame}{Arquitectura GPT}
    \begin{itemize}
        \item<1> Limitaciones de las arquitecturas recurrentes.
        \item<2> Layer normalization.
        \item<3> Self-attention: masking, atención multicabezal.
        \item<4> Red feed forward
        \item<5> Bloque Transformer.
        \item<6> Clase GPT.
    \end{itemize}
\end{frame}

\begin{frame}{Entrenamiento y generación}
    \begin{itemize}
        \item<1> Entrenamiento.
        \item<2> Generación.
        \item<3> Visualizacion matrices de atención.
    \end{itemize}
\end{frame}

\begin{frame}{Próxima clase}
    En la próxima clase.
    \begin{itemize}
        \item<1> Algunas cosas sobre los LLMs y la arquitectura Transformer.
    \end{itemize}
\end{frame}

\begin{frame}
    \centering
    \Large{Modelos Generativos Profundos}\\
    \large{Clase 6: Implementación de un modelo GPT}
\end{frame}

\end{document}