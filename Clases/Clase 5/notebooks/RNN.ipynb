{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de texto con una RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import re\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self, corpus_vocabulary):\n",
    "        self.vocabulary = corpus_vocabulary + ['<BOS>', '<EOS>', '<UNK>', '<PAD>']\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "\n",
    "        self.token_to_id = {token: id for id, token in enumerate(self.vocabulary)}\n",
    "        self.bos_id = self.token_to_id['<BOS>']\n",
    "        self.eos_id = self.token_to_id['<EOS>']\n",
    "        self.unk_id = self.token_to_id['<UNK>']\n",
    "        self.pad_id = self.token_to_id['<PAD>']\n",
    "\n",
    "    def encode(self, text):\n",
    "        seq_tokens = re.findall(r'\\d|[^\\w\\s]|\\w+|\\s', text)\n",
    "        seq_ids = [self.token_to_id.get(token, self.unk_id) for token in seq_tokens]\n",
    "        return seq_ids\n",
    "\n",
    "    def decode(self, seq_ids):\n",
    "        seq_tokens = ''.join(self.vocabulary[i] for i in seq_ids)\n",
    "        return seq_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso:\n",
    "\n",
    "vocabulary = ['a', 'b', 'c', 'd', ' ']\n",
    "tokenizer = Tokenizer(vocabulary)\n",
    "\n",
    "print(f'<BOS> ID: {tokenizer.bos_id}.')\n",
    "print(f'<EOS> ID: {tokenizer.eos_id}.')\n",
    "print(f'<UNK> ID: {tokenizer.unk_id}.')\n",
    "print(f'<PAD> ID: {tokenizer.pad_id}.')\n",
    "\n",
    "token_seq = 'a b c d e'\n",
    "id_seq = tokenizer.encode(token_seq)\n",
    "print(f'encode(\"{token_seq}\") = {id_seq}.')\n",
    "print(f'decode({id_seq}) = \"{tokenizer.decode(id_seq)}\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, filename, seq_length):\n",
    "\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            corpus = file.read()\n",
    "\n",
    "        corpus_vocabulary = sorted(set(re.findall(r'\\d|[^\\w\\s]|\\w+|\\s', corpus)))\n",
    "        self.tokenizer = Tokenizer(corpus_vocabulary)\n",
    "\n",
    "        sentences = [sentence.strip() for sentence in corpus.split('\\n') if sentence.strip()]\n",
    "        self.data = [[self.tokenizer.bos_id] + self.tokenizer.encode(sentence) + [self.tokenizer.eos_id] for sentence in sentences]\n",
    "\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "\n",
    "        seq_ids = self.data[n]\n",
    "\n",
    "        if len(seq_ids) > self.seq_length:\n",
    "            seq_ids = seq_ids[:self.seq_length]\n",
    "        else:\n",
    "            seq_ids += [self.tokenizer.pad_id] * (self.seq_length - len(seq_ids))\n",
    "\n",
    "        return torch.tensor(seq_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length, batch_size = 256, 32\n",
    "\n",
    "dataset = TextDataset('data.txt', seq_length)\n",
    "dataloader = DataLoader(dataset, batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "print(f'Tamaño del dataset: {len(dataset)} secuencias.')\n",
    "print(f'Tamaño del dataloader: {len(dataloader)} batches.')\n",
    "print(f'Tamaño del vocabulario: {dataset.tokenizer.vocab_size} tokens.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso:\n",
    "\n",
    "seq_ids = dataset[0]\n",
    "seq_tokens = dataset.tokenizer.decode(seq_ids[:40])\n",
    "print(seq_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso:\n",
    "\n",
    "vocab_size, embedding_dim = dataset.tokenizer.vocab_size, 384\n",
    "emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "x = torch.randint(vocab_size, (batch_size, seq_length))\n",
    "y = emb(x)\n",
    "\n",
    "assert y.shape == (batch_size, seq_length, embedding_dim)\n",
    "assert emb.weight.shape == (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red recurrente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, embedding_dim, batch_first=True)\n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_embedding = self.embedding(x.long())  # [batch_size, seq_length, embedding_dim].\n",
    "        rnn_output, _ = self.rnn(seq_embedding)   # [batch_size, seq_length, embedding_dim], [1, batch_size, embedding_dim].\n",
    "        logits = self.lm_head(rnn_output)         # (batch_size, seq_length, vocab_size].\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso:\n",
    "\n",
    "vocab_size, embedding_dim = dataset.tokenizer.vocab_size, 384\n",
    "rnn = GenerativeRNN(vocab_size, embedding_dim)\n",
    "\n",
    "x = torch.randint(vocab_size, size=[batch_size, seq_length])\n",
    "y = rnn(x)\n",
    "\n",
    "assert y.shape == (batch_size, seq_length, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red que se usará:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = GenerativeRNN(dataset.tokenizer.vocab_size, embedding_dim=384)\n",
    "\n",
    "n_params = sum(param.numel() for param in rnn.parameters()) / 1e6\n",
    "print(f'Cantidad de parámetros: {n_params:.3} millones.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equivalencia entre verosimilitud normalizada y CELoss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_length, vocab_size = 32, 256, dataset.tokenizer.vocab_size\n",
    "\n",
    "logits = torch.randn(batch_size, seq_length, vocab_size)\n",
    "targets = torch.randint(0, vocab_size, size=(batch_size, seq_length))\n",
    "\n",
    "# Cálculo de la función de pérdida paso a paso:\n",
    "probs = logits.softmax(dim=-1)\n",
    "target_probs = probs.gather(dim=-1, index=targets.unsqueeze(-1)).squeeze(-1)  # [batch_size, seq_length].\n",
    "\n",
    "log_likelihood = target_probs.log().sum(dim=-1)  # [batch_size].\n",
    "normalized_log_likelihood = log_likelihood / seq_length\n",
    "manual_loss = - normalized_log_likelihood.mean()\n",
    "\n",
    "# Cálculo de la función de pérdida con CrossEntropyLoss:\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "logits_reshaped = logits.view(batch_size * seq_length, vocab_size)\n",
    "targets_reshaped = targets.view(batch_size * seq_length)\n",
    "direct_loss = loss_fn(logits_reshaped, targets_reshaped)\n",
    "\n",
    "assert torch.isclose(manual_loss, direct_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, dataloader, epochs, ckpt_filename):\n",
    "\n",
    "    model.to(DEVICE)\n",
    "    model.train()\n",
    "\n",
    "    pad_id = dataloader.dataset.tokenizer.pad_id\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "\n",
    "    training = {'losses': [], 'model': None}\n",
    "\n",
    "    try:\n",
    "        progressbar = tqdm.trange(epochs)\n",
    "        for epoch in progressbar:\n",
    "\n",
    "            for seq_batch in dataloader:\n",
    "\n",
    "                seq_batch = seq_batch.to(DEVICE)  # [batch_size, seq_length].\n",
    "                x_batch, y_batch = seq_batch[:, :-1], seq_batch[:, 1:]\n",
    "\n",
    "                logits = model(x_batch)\n",
    "\n",
    "                batch_size, seq_length, vocab_size = logits.shape\n",
    "                logits = logits.reshape(batch_size * seq_length, vocab_size)\n",
    "                y_batch = y_batch.reshape(batch_size * seq_length)\n",
    "\n",
    "                loss = loss_fn(logits, y_batch)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                training['losses'].append(loss.item())\n",
    "                progressbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('Entrenamiento interrumpido.')\n",
    "\n",
    "    training['model'] = model.state_dict()\n",
    "    torch.save(training, ckpt_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_optimizer = optim.AdamW(rnn.parameters())\n",
    "#train_model(rnn, rnn_optimizer, dataloader, epochs=32, ckpt_filename='rnn_training.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_training = torch.load('rnn_training.pt', DEVICE, weights_only=True)\n",
    "rnn.load_state_dict(rnn_training['model'])\n",
    "\n",
    "plt.plot(rnn_training['losses'])\n",
    "plt.xlabel('Iteración')\n",
    "plt.ylabel('Entropía cruzada media')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso:\n",
    "\n",
    "vocab_size = 10\n",
    "vocabulary = [f'$a_{{{k}}}$' for k in range(1, vocab_size + 1)]\n",
    "logits = torch.randn(vocab_size)\n",
    "temperatures = [0.1, 0.5, 1, 2, 5, 10]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(10, 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, t in enumerate(temperatures):\n",
    "    scaled_logits = logits / t\n",
    "    prob = scaled_logits.softmax(-1)\n",
    "    axes[i].bar(vocabulary, prob)\n",
    "    axes[i].set_title(f'$T={t}$')\n",
    "    axes[i].set_ylabel(f'Probabilidad')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop de generación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokens(model, context, tokenizer, temperature=1, top_k=50, max_tokens=512, repetition_penalty=1):\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    seq_id = [tokenizer.bos_id] + tokenizer.encode(context)\n",
    "    seq_id = torch.tensor(seq_id, device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_tokens):\n",
    "            logits = model(seq_id.unsqueeze(0))[0, -1, :]\n",
    "\n",
    "            if temperature == 0:\n",
    "                next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "            else:\n",
    "                logits = logits / temperature\n",
    "\n",
    "                token_counts = torch.bincount(seq_id, minlength=logits.size(0))\n",
    "                for token_id, count in enumerate(token_counts):\n",
    "                    if count > 0:\n",
    "                        logits[token_id] /= (repetition_penalty ** count)\n",
    "\n",
    "                top_k_logits, top_k_indices = torch.topk(logits, top_k)\n",
    "                probs = top_k_logits.softmax(dim=-1)\n",
    "                next_token = top_k_indices[torch.multinomial(probs, num_samples=1)]\n",
    "\n",
    "            seq_id = torch.cat((seq_id, next_token), dim=0)\n",
    "\n",
    "            if next_token in (tokenizer.eos_id, tokenizer.pad_id):\n",
    "                break\n",
    "\n",
    "    return tokenizer.decode(seq_id.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = 'Habia una vez'\n",
    "\n",
    "for n in range(10):\n",
    "    new_tokens = generate_tokens(rnn, context, dataset.tokenizer, max_tokens=64)\n",
    "    print(new_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
