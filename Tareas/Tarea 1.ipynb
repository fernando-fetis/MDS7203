{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APEcMpP3ru2r"
      },
      "source": [
        "# Tarea 1: Modelos Autorregresivos\n",
        "\n",
        "### MDS7203 Modelos Generativos Profundos\n",
        "\n",
        "**Nombre:**\n",
        "\n",
        "**Fecha de entrega:**\n",
        "\n",
        "Dado que la arquitectura Transformer es una de las dos arquitecturas específicas que se verán a lo largo del curso (la otra es la arquitectura U-Net), es importante asegurarse de entenderla bien. Por este motivo, esta primera tarea tendrá varias preguntas conceptuales que buscan evaluar si se entienden bien algunos detalles importantes de la arquitectura Transformer.\n",
        "\n",
        "Algunas instrucciones generales:\n",
        "- Se pueden utilizar de manera libre herramientas como ChatGPT y Claude, entre otras.\n",
        "- Para la entrega, no es necesario un informe, este archivo es suficiente.\n",
        "- Se debe entregar el documento con todas las celdas ejecutadas.\n",
        "- Se recomienda correr la tarea en GPU (usando Google Colab o una GPU local).\n",
        "- La tarea tiene 3 partes, y cada parte vale lo mismo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDFZAn8Qru2t"
      },
      "source": [
        "## Parte 1 (preguntas conceptuales)\n",
        "\n",
        "### Modelos autorregresivos generales\n",
        "- ¿Qué son los tokens especiales dentro de un vocabulario? Nombre al menos 5 junto a su función.\n",
        "> **Respuesta:**\n",
        "- Los modelos autorregresivos se entrenan siguiendo el enfoque de máxima verosimilitud. Sin embargo, durante el entrenamiento, es usual normalizar la verosimilitud de una secuencia por su largo, ¿por qué se hace esto?\n",
        "> **Respuesta:**\n",
        "- ¿Por qué los modelos neuronales por lo general no aplican la función Softmax en la salida, incluso cuando se está buscando aprender un vector de probabilidades (que permite definir una distribución sobre el vocabulario para predecir el siguiente token)? ¿Cómo se relaciona esto con la idea de temperatura usada durante la inferencia?\n",
        "> **Respuesta:**\n",
        "- ¿Cuál es la función de la temperatura al generar secuencias? ¿Qué diferencia hay entre una temperatura alta y una temperatura baja?\n",
        "> **Respuesta:**\n",
        "\n",
        "### Arquitectura GPT\n",
        "- ¿Por qué se proyecta una misma entrada en 3 matrices ($Q$, $K$ y $V$) para el cálculo de atención? ¿Por qué no usar la propia entrada como vectores de query, key y value?\n",
        "> **Respuesta:**\n",
        "- Al hacer el cálculo de atención, ¿por qué se normaliza el producto punto por la dimensión de los vectores de query y key?\n",
        "> **Respuesta:**\n",
        "- ¿Cuál es la función del masking causal al calcular self-attention? ¿Por qué no se aplica masking causal en el modelo BERT? ¿Qué tipo de masking utiliza BERT?\n",
        "> **Respuesta:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1VQb3-Qru2u"
      },
      "source": [
        "## Parte 2 (instruction fine tuning)\n",
        "\n",
        "Esta parte tiene dos objetivos principales:\n",
        "- Entender cómo se realiza instruction fine tuning (IFT). Esto busca evaluar que se entiende el proceso de entrenamiento de un LLM.\n",
        "- Implementar KV caching para acelerar la inferencia de un LLM. Esto busca evaluar que se entiende el proceso de generación de un LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGYiDPRXru2u",
        "outputId": "64e518a5-d175-4628-c5e3-3600a7eb2c6b"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken\n",
        "\n",
        "import torch\n",
        "import time\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import json\n",
        "import tiktoken\n",
        "import gdown\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NxVEk93ru2v"
      },
      "source": [
        "Se comenzará descargando los datos de entrenamiento y los parámetros de un modelo ya pre-entrenado (GPT 2):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "OBv36uq3ru2v",
        "outputId": "d2a69441-7347-4725-cc6f-056ca06ebc8c"
      },
      "outputs": [],
      "source": [
        "datos = 'https://drive.google.com/file/d/1Wky_HjclMRrg92bQu-xSp86PrdxLzlZE/view?usp=share_link'\n",
        "output_path = 'data.json'\n",
        "gdown.download(datos, output_path, quiet=False,fuzzy=True)\n",
        "\n",
        "modelo = 'https://drive.google.com/file/d/1Xhh4G2HpR7vTsUx_-FmEfOt7OG9rD3wM/view?usp=share_link'\n",
        "output_path = 'gpt2.pt'\n",
        "gdown.download(modelo, output_path, quiet=False,fuzzy=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UmEvQ2qru2v"
      },
      "source": [
        "### Tokenización\n",
        "\n",
        "Para la tokenización, se utilizará la librería `tiktoken` desarrollada por OpenAI. Esta librería contiene el tokenizador usado por el modelo GPT 2, al cual se le aplicará instruction fine tuning.\n",
        "\n",
        "- ¿Cuál es el tokenizador que utiliza GPT 2? Indique sus principales ventajas sobre tokenizar palabra a palabra.\n",
        "\n",
        "> **Respuesta:**\n",
        "\n",
        "- Notar que este tokenizador utiliza un mismo token como token \\<PAD\\> y token \\<EOS\\>, ¿por qué se puede hacer esto?\n",
        "\n",
        "> **Respuesta:**\n",
        "\n",
        "Se implementará una clase `Tokenizer` con una estructura similar a la implementada en clases:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCYmBcAwhuzi"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.tokenizer = tiktoken.get_encoding('gpt2')\n",
        "        self.eos_id = 50256\n",
        "        self.pad_id = 50256\n",
        "\n",
        "    def encode(self, text):\n",
        "        return self.tokenizer.encode(text)\n",
        "\n",
        "    def decode(self, seq_ids):\n",
        "        return self.tokenizer.decode(seq_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1P41Dg_ru2w"
      },
      "source": [
        "### Dataset de entrenamiento\n",
        "\n",
        "Los datos de entrenamiento están almacenados en un archivo JSON, donde cada entrada es de la forma\n",
        "\n",
        "```\n",
        "{\n",
        "    \"instruction\": <texto con instrucciones>\n",
        "    \"input\":       <texto de entrada (opcional)>\n",
        "    \"output\":      <respuesta>\n",
        "}\n",
        "```\n",
        "\n",
        "La idea del IFT es continuar el entrenamiento de un modelo autorregresivo pre-entrenado (en este caso, se usará GPT 2), donde cada muestra de entrenamiento es una secuencia de texto que contiene:\n",
        "\n",
        "- **Instrucción:** contexto para el modelo. Aquí se le puede indicar que es un asistente que responde consultas hechas en el input.\n",
        "- **Input:** indica la _pregunta_ que el modelo debe responder. Muchas veces, el input es omitido ya que la pregunta se incluye en la instrucción.\n",
        "- **Output:** respuesta que se espera que entregue el modelo.\n",
        "\n",
        "Para entregarle esta información al modelo, es usual construir *instruction templates*, los cuales unen los 3 elementos en una única secuencia de tokens (*prompt*) con una estructura definida.\n",
        "\n",
        "- Implemente el método `format_input` en la clase `InstructionDataset` siguiendo el template Alpaca."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1suuccyYB8KL"
      },
      "outputs": [],
      "source": [
        "class InstructionDataset(Dataset):\n",
        "\n",
        "    def __init__(self, filename, tokenizer):\n",
        "\n",
        "        with open(filename, 'r', encoding='utf-8') as file:\n",
        "            self.data = json.load(file)\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.encoded_sequences = []\n",
        "\n",
        "        for sample in self.data:\n",
        "            prompt, response = self.format_input(sample)\n",
        "            full_text = prompt + response\n",
        "            token_ids = tokenizer.encode(full_text)\n",
        "            token_ids.append(tokenizer.eos_id)\n",
        "            self.encoded_sequences.append(token_ids)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoded_sequences)\n",
        "\n",
        "    def __getitem__(self, n):\n",
        "        return self.encoded_sequences[n]\n",
        "\n",
        "    @staticmethod\n",
        "    def format_input(sample):\n",
        "        ...\n",
        "        return prompt, response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_stTo3Ukru2w"
      },
      "source": [
        "Se visualizarán algunos ejemplos de muestras que entrega `InstructionDataset`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-P16NTChuzi",
        "outputId": "6478a1b7-2290-4916-8d35-e8b81399d357"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "dataset = InstructionDataset('data.json', tokenizer)\n",
        "\n",
        "print(f'Tamaño dataset:', len(dataset))\n",
        "\n",
        "for i in range(5):\n",
        "    print('-' * 100)\n",
        "    print(tokenizer.decode(dataset[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF-BQLlfru2x"
      },
      "source": [
        "### Dataloader\n",
        "\n",
        "Dado que el entrenamiento se realiza utilizando batches de secuencias, todas las secuencias deben ser del mismo largo, por lo que las secuencias muy largas se truncan a un largo fijo, mientras que las secuencias muy cortas se extienden hasta el mismo largo fijo. En clases, este preprocesamiento se implementó dentro de la clase asociada al dataset. En este caso, el preprocesamiento se definirá al instanciar el dataloader usando una _collate function_ (función `collate_fn` implementada en la siguiente celda):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UU46WjTpB8KM"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "\n",
        "    max_len = max(len(seq) for seq in batch)\n",
        "\n",
        "    padded_batch = []\n",
        "    for seq in batch:\n",
        "        if len(seq) < max_len:\n",
        "            seq = seq + [tokenizer.eos_id] * (max_len - len(seq))\n",
        "        else:\n",
        "            seq = seq[:max_len]\n",
        "        padded_batch.append(seq)\n",
        "\n",
        "    padded_tensor = torch.tensor(padded_batch, dtype=torch.long)\n",
        "\n",
        "    return padded_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJf7YcaiB8KN",
        "outputId": "17e61a3b-8916-4112-a280-cfd57c2dc496"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Ejemplo:\n",
        "batch1 = next(iter(dataloader))\n",
        "batch2 = next(iter(dataloader))\n",
        "print(batch1.shape, batch2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- ¿Cuál es la principal ventaja de realizar este cambio?\n",
        "\n",
        ">**Respuesta:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIqq9AKOhuzj"
      },
      "source": [
        "### Arquitectura GPT\n",
        "\n",
        "La siguiente celda contiene una implementación del modelo GPT 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTRuXt3oB8KO"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        new_seq_length = queries.shape[2]\n",
        "        mask = torch.triu(torch.ones(new_seq_length, new_seq_length, device=x.device), diagonal=1).bool()\n",
        "        attn_scores = queries @ keys.transpose(2, 3)\n",
        "        attn_scores.masked_fill_(mask, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / (keys.shape[-1] ** 0.5), dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        attn_output = self.att(x)\n",
        "        x = self.drop_resid(attn_output)\n",
        "        x = x + shortcut\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_resid(x)\n",
        "        x = x + shortcut\n",
        "\n",
        "        return x\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_indices = torch.arange(seq_len, device=in_idx.device)\n",
        "        pos_embeds = self.pos_emb(pos_indices)\n",
        "        x = tok_embeds + pos_embeds\n",
        "        x = self.drop_emb(x)\n",
        "\n",
        "        for block in self.trf_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n7foQP-ru2z"
      },
      "source": [
        "Notar que esta implementación tiene algunas diferencias menores con la implementación vista en clases. Por ejemplo, se realiza el cálculo de MHSA de forma directa (convencerse de que la clase `MultiHeadAttention` realiza lo esperado) y también se consideran términos de bias en las capas lineales (aunque trabajos posteriores muestran que no usar términos de bias estabiliza el entrenamiento en modelos más grandes).\n",
        "\n",
        "Se inicializará el modelo GPT 2 con los parámetros descargados anteriormente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y5OL7Dghuzo",
        "outputId": "910d353a-d46a-41af-821a-d303335ccb58"
      },
      "outputs": [],
      "source": [
        "# Arquitectura GPT:\n",
        "\n",
        "cfig = {'vocab_size': 50257,\n",
        " 'context_length': 1024,\n",
        " 'drop_rate': 0.0,\n",
        " 'qkv_bias': True,\n",
        " 'emb_dim': 1024,\n",
        " 'n_layers': 24,\n",
        " 'n_heads': 16}\n",
        "\n",
        "gpt = GPTModel(cfig)\n",
        "gpt.to(DEVICE)\n",
        "\n",
        "# Inicialización:\n",
        "params = torch.load('gpt2.pt', map_location=DEVICE)\n",
        "gpt.load_state_dict(params)\n",
        "\n",
        "# Cantidad de parámetros:\n",
        "n_params = sum(param.numel() for param in gpt.parameters()) / 1e6\n",
        "print(f'Cantidad de parámetros: {n_params:.4} millones.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjjM7zXYhuzo"
      },
      "source": [
        "### Entrenamiento\n",
        "\n",
        "Para el entrenamiento (fine tuning) se utilizará el mismo loop definido en clases:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OCeHErnru20"
      },
      "outputs": [],
      "source": [
        "def train_model(model, optimizer, dataloader, epochs, ckpt_filename):\n",
        "\n",
        "    model.to(DEVICE)\n",
        "    model.train()\n",
        "\n",
        "    pad_id = dataloader.dataset.tokenizer.pad_id\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
        "\n",
        "    training = {'losses': [], 'model': None}\n",
        "\n",
        "    try:\n",
        "        progressbar = tqdm.trange(epochs)\n",
        "        for epoch in progressbar:\n",
        "\n",
        "            for seq_batch in dataloader:\n",
        "\n",
        "                seq_batch = seq_batch.to(DEVICE)\n",
        "                x_batch, y_batch = seq_batch[:, :-1], seq_batch[:, 1:]\n",
        "\n",
        "                logits = model(x_batch)\n",
        "\n",
        "                batch_size, seq_length, vocab_size = logits.shape\n",
        "                logits = logits.reshape(batch_size * seq_length, vocab_size)\n",
        "                y_batch = y_batch.reshape(batch_size * seq_length)\n",
        "\n",
        "                loss = loss_fn(logits, y_batch)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                training['losses'].append(loss.item())\n",
        "                progressbar.set_postfix(loss=loss.item())\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print('Entrenamiento interrumpido.')\n",
        "\n",
        "    training['model'] = model.state_dict()\n",
        "    torch.save(training, ckpt_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se entrenará el modelo sobre el dataset definido anteriormente. Para esto, se usará, como es usual, el optimizador Adam:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3dalQJvhuzo",
        "outputId": "e2c7b7d9-9dcb-4254-ab11-cff9ada68eec"
      },
      "outputs": [],
      "source": [
        "gpt_optimizer = optim.AdamW(gpt.parameters())\n",
        "train_model(gpt, gpt_optimizer, dataloader, epochs=2, ckpt_filename='gpt2_ift.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "gwSJZ-9Whuzo",
        "outputId": "7c18350f-6d29-4c09-faa0-cd7782cba99b"
      },
      "outputs": [],
      "source": [
        "gpt_training = torch.load('gpt2_ift.pt', DEVICE, weights_only=True)\n",
        "gpt.load_state_dict(gpt_training['model'])\n",
        "\n",
        "plt.plot(gpt_training['losses'], label='train loss')\n",
        "plt.xlabel('Iteración')\n",
        "plt.ylabel('Entropía cruzada')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CylxOzsPru21"
      },
      "source": [
        "Notar que no se está considerando un dataset de evaluación, lo cual es imprescindible para evaluar el overfitting del modelo. En esta tarea se omitirá la evaluación por simplicidad, pero es importante tener presente que siempre debe realizarse en proyectos reales.\n",
        "\n",
        "- ¿Por qué es malo para un LLM que se produzca overfitting?\n",
        "\n",
        "> **Respuesta:**\n",
        "\n",
        "- ¿De qué forma se puede evaluar un LLM?\n",
        "\n",
        "> **Respuesta:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE1F1tWUhuzo"
      },
      "source": [
        "### Generación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49Nz3Oetru21"
      },
      "source": [
        "Teniendo el modelo entrenado, se puede utilizar para hacer inferencia. El loop de generación que se usará es el mismo que se vio en clases:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZV6q6OaaOvg"
      },
      "outputs": [],
      "source": [
        "def generate_tokens(model, context, tokenizer, temperature=1, top_k=50, max_tokens=512, repetition_penalty=1):\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "\n",
        "    seq_id = tokenizer.encode(context)\n",
        "    seq_id = torch.tensor(seq_id, device=DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_tokens):\n",
        "            logits = model(seq_id.unsqueeze(0))[0, -1, :]\n",
        "\n",
        "            if temperature == 0:\n",
        "                next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "            else:\n",
        "                logits = logits / temperature\n",
        "\n",
        "                token_counts = torch.bincount(seq_id, minlength=logits.size(0))\n",
        "                for token_id, count in enumerate(token_counts):\n",
        "                    if count > 0:\n",
        "                        logits[token_id] /= (repetition_penalty ** count)\n",
        "\n",
        "                top_k_logits, top_k_indices = torch.topk(logits, top_k)\n",
        "                probs = top_k_logits.softmax(dim=-1)\n",
        "                next_token = top_k_indices[torch.multinomial(probs, num_samples=1)]\n",
        "\n",
        "            seq_id = torch.cat((seq_id, next_token), dim=0)\n",
        "\n",
        "            if next_token in (tokenizer.eos_id, tokenizer.pad_id):\n",
        "                break\n",
        "\n",
        "    return tokenizer.decode(seq_id.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB6SMpQdru21"
      },
      "source": [
        "Para evaluar cualitativamente el entrenamiento, se comparará la respuesta generada por el modelo con la respuesta real utilizando una muestra del train set. Notar que el modelo puede estar sobreajustado, por lo que puede haber memorizado la respuesta. Sin embargo, en esta tarea no nos preocuparemos de eso (para evitar overfitting, basta con entrenar sobre más datos y durante más tiempo, siguiendo las leyes de escala de los LLMs).\n",
        "\n",
        "- Ajuste los hiperparámetros de generación (`temperature`, `top_k`, `max_tokens` y `repetition_penalty`) para obtener resultados aceptables. Proponga una hipótesis de por qué los hiperparámetros elegidos generan buenos resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EA_il9UQD5EL",
        "outputId": "a4585955-a2a4-4b7a-b529-9577bebea647"
      },
      "outputs": [],
      "source": [
        "temperature = ...\n",
        "top_k = ...\n",
        "max_tokens = ...\n",
        "repetition_penalty = ...\n",
        "\n",
        "i = torch.randint(100, size=[1]).item()\n",
        "\n",
        "prompt, real_response = dataset.format_input(dataset.data[i])\n",
        "output = generate_tokens(gpt, prompt, dataset.tokenizer, temperature, top_k, max_tokens, repetition_penalty)\n",
        "\n",
        "print(output)\n",
        "print(\"-\" * 80 + \"\\n\")\n",
        "print(f'RESPUESTA REAL: {real_response}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmMgT_Ugru22"
      },
      "source": [
        "## Parte 3 (KV caching)\n",
        "\n",
        "En clases se revisó la técnica de KV caching, la cual modifica la implementación de un modelo para poder realizar el cálculo de atención de manera más eficiente durante la inferencia. Esta técnica consiste en almacenar temporalmente los vectores de key y value de los tokens procesados anteriormente para así no tener que calcular las proyecciones $K$ y $V$ necesarias para el cálculo de atención.\n",
        "\n",
        "En esta pregunta se debe modificar el código anterior para habilitar la opción de hacer KV caching. Para esto, se sugiere lo siguiente:\n",
        "- Modificar los módulos de la arquitectura GPT para que el modelo pueda recibir vectores de key y value calculados anteriormente y, así, utilizarlo en el cálculo de atención. De esta forma, el modelo debería retornar una tupla de la forma `output, (keys, values)`, donde `output` es la salida original del modelo (lo que retorna actualmente) y `(keys, values)` son las matrices de query y value actualizadas (se agrega una fila con los vectores recién calculados).\n",
        "- Modificar la función `generate_tokens` para que realice la generación utilizando la técnica de KV caching utilizando las matrices `(keys, values)` que se van actualizando en cada iteración.\n",
        "\n",
        "Notar que KV caching no se utiliza durante el entrenamiento. Por otro lado, es importante ver que solo es necesario modificar los métodos `forward` de algunos módulos y no los métodos `__init__`. En particular, esto no alterará la cantidad de parámetros, por lo que se puede cargar el modelo entrenado en la pregunta anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ft8lSg7Sru22"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x, past_key_value=None):\n",
        "        ...\n",
        "        return output, (keys, values)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x, past_key_value=None):\n",
        "        ...\n",
        "        return output, (keys, values)\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, in_idx, past_key_values=None):\n",
        "        ...\n",
        "        return output, (keys, values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZL5qy5aIe1M"
      },
      "source": [
        "Se cargará el modelo entrenado sobre esta nueva arquitectura. Dado que no se cambió el número de parámetros, los pesos se pueden cargar sin problemas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiOgs2DSru22",
        "outputId": "69d05106-c3e7-4bde-ba08-d445699c186f"
      },
      "outputs": [],
      "source": [
        "# Arquitectura GPT:\n",
        "gpt_kvcaching = GPTModel(cfig)\n",
        "gpt_kvcaching.to(DEVICE)\n",
        "\n",
        "# Carga del modelo entrenado:\n",
        "gpt_training = torch.load('gpt2_ift.pt', DEVICE, weights_only=True)\n",
        "gpt_kvcaching.load_state_dict(gpt_training['model'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3GubMdyItJD"
      },
      "source": [
        "Ahora se debe modificar la función `generate_tokens` para utilizar KV caching:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXa_y7jpru22"
      },
      "outputs": [],
      "source": [
        "def generate_tokens_kvcaching(model, context, tokenizer, temperature=1, top_k=50, max_tokens=512, repetition_penalty=1):\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H573UriI3Sz"
      },
      "source": [
        "Para ver la diferencia, se generarán muestras utilizando ambos modelos (sin KV caching y con KV caching). Si las cosas quedaron bien implementadas, debería haber una diferencia significativa de rendimiento, la cual se vuelve más notoria a medida que aumenta el largo de las secuencias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OztcRhdsHrZ0",
        "outputId": "41fea6da-cb51-4093-934f-462dd3494593"
      },
      "outputs": [],
      "source": [
        "i = torch.randint(100, size=[1]).item()\n",
        "\n",
        "prompt, _ = dataset.format_input(dataset.data[i])\n",
        "\n",
        "# Sin KV caching:\n",
        "start_time = time.time()\n",
        "output = generate_tokens(gpt, prompt, dataset.tokenizer, max_tokens=64)\n",
        "end_time = time.time()\n",
        "generation_frequency = (len(output) - len(prompt)) / (end_time - start_time)\n",
        "print(f'[Frecuencia de generación (sin KV caching): {generation_frequency:.4f} tokens/segundo]')\n",
        "\n",
        "# Usando KV caching:\n",
        "start_time = time.time()\n",
        "output = generate_tokens_kvcaching(gpt_kvcaching, prompt, dataset.tokenizer, max_tokens=64)\n",
        "end_time = time.time()\n",
        "generation_frequency = (len(output) - len(prompt)) / (end_time - start_time)\n",
        "print(f'[Frecuencia de generación (con KV caching): {generation_frequency:.4f} tokens/segundo]')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
